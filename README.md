# ğŸ¥ Medizinischer Chatbot mit RAG

Ein medizinischer Chatbot basierend auf **TinyLlama** und **Llama 3.1**, kombiniert mit **Retrieval-Augmented Generation (RAG)** fÃ¼r prÃ¤zise, kontextbasierte Antworten.

---

## ğŸŒŸ Features
- ğŸ’¬ **Chatbot im Browser** (Gradio)
- ğŸ§  **RAG-basiert**: Lokales Wissen + Cloud-Generierung
- ğŸ“š **Lernmodus**: Speichert neue Erkenntnisse aus GesprÃ¤chen
- ğŸ› ï¸ **Admin-Interface**: Verwalte die Wissensdatenbank
- ğŸ”’ **Datenschutz**: Lokales Retrieval, keine externen Speicherungen

---

## ğŸ§° Voraussetzungen
- Python **3.10+**
- Mindestens **8 GB RAM**
- (Optional) GPU fÃ¼r bessere Performance
- Hugging Face Account + Token: [Einstellungen â†’ Tokens](https://huggingface.co/settings/tokens)

---

## ğŸš€ Installation & Schnellstart

### 1. Repository klonen
```bash
git clone <dein-repo-link>
cd medical_chatbot
```

### 2. Virtuelle Umgebung erstellen
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 3. AbhÃ¤ngigkeiten installieren
```bash
pip install -r requirements.txt
```

### 4. Hugging Face Token setzen
```bash
export HF_TOKEN=hf_xxxxxxxxxxxxxxxx   # Linux/Mac
setx HF_TOKEN hf_xxxxxxxxxxxxxxxx     # Windows
```

### 5. Anwendung starten
- ğŸŒ **Webinterface**  
  ```bash
  python web_rag_llama3.py
  ```

- ğŸ› ï¸ **Admin-Interface**  
  ```bash
  python admin_interface.py
  ```

---

## ğŸ§ª Nutzung

### Chatbot-Befehle
- Normale Frage: `Was ist eine Grippe?`
- Wissen speichern: `lerne das` nach einer Antwort

### Admin-Interface
- Alle EintrÃ¤ge anzeigen  
- Neue EintrÃ¤ge hinzufÃ¼gen  
- EintrÃ¤ge lÃ¶schen  
- In Wissensdatenbank suchen  

---

## ğŸ“ Projektstruktur
```
medical_chatbot/
â”œâ”€â”€ README.md               # Diese Datei
â”œâ”€â”€ requirements.txt        # AbhÃ¤ngigkeiten
â”œâ”€â”€ medical_data.json       # Beispieldaten
â”œâ”€â”€ train_medical_bot.py    # Optionales Training
â”œâ”€â”€ chatbot_cli.py          # Konsolen-Chatbot
â”œâ”€â”€ chatbot_web.py          # Einfaches Webinterface
â”œâ”€â”€ hybrid_rag.py           # RAG mit lokalem Retrieval
â”œâ”€â”€ web_rag_llama3.py       # Haupt-Webinterface (Llama 3.1)
â”œâ”€â”€ admin_interface.py      # Admin-Tool
â””â”€â”€ chroma_db/              # Lokale Vektordatenbank
```

---

## ğŸ§  Modellinformationen
- **Embedding**: `all-MiniLM-L6-v2` (lokal)  
- **Generativ**: `meta-llama/Llama-3.1-8B-Instruct` (Cloud)  
- **Optionales Pretraining**: `SpookyFab/tinyllama-pretrained-custom`  

ğŸ‘‰ FÃ¼r Zugriff: [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) Ã¶ffnen und Nutzungsbedingungen akzeptieren.

---

## ğŸ“š Datenquellen
- Ã–ffentliche medizinische DatensÃ¤tze  
- Wikipedia (Medizin-Kapitel)  
- Eigene Erfahrungen Ã¼ber Lernmodus  

---

## ğŸ›¡ï¸ Datenschutz
- Retrieval lÃ¤uft **lokal**  
- Fragen werden nicht dauerhaft gespeichert  
- Antworten werden nicht extern archiviert  
- Lernmodus speichert nur explizit genehmigte Inhalte  

---

## ğŸ§¾ NÃ¤chste Schritte
- Erweiterung mit mehr medizinischen Daten  
- Experten-Bots fÃ¼r weitere Fachgebiete (Recht, Technik, etc.)  
- Deployment in der Cloud (AWS, Google Cloud)  
- API-Schnittstelle bereitstellen  

---

## ğŸ¤ Mitwirken
1. Repo forken  
2. Branch erstellen: `feature/NeueFunktion`  
3. Ã„nderungen committen  
4. Pull Request Ã¶ffnen  

---

## ğŸ“„ Lizenz
MIT License â€“ siehe [LICENSE](LICENSE)  

---

## ğŸ“ Kontakt
Fragen oder Probleme? â†’ [Issue erstellen](../../issues)
