# ğŸ¥ Medizinischer Chatbot mit RAG

Ein fortschrittlicher, medizinischer Chatbot basierend auf **TinyLlama** und **Llama 3.1**, kombiniert mit **Retrieval-Augmented Generation (RAG)** fÃ¼r prÃ¤zise, kontextbasierte Antworten.

## ğŸŒŸ Funktionen

- ğŸ’¬ **Chatbot im Browser** (Gradio)
- ğŸ§  **RAG-basiert** (lokales Wissen + Cloud-Generierung)
- ğŸ“š **Lernmodus**: Speichert neue Erkenntnisse aus GesprÃ¤chen
- ğŸ› ï¸ **Admin-Interface**: Verwalte die Wissensdatenbank
- ğŸ”’ **Datenschutz**: Lokales Retrieval, keine externen Speicherungen

## ğŸ§° Voraussetzungen

- Python 3.10+
- Mindestens 8 GB RAM
- (Optional) GPU fÃ¼r bessere Performance

## ğŸš€ Schnellstart

### 1. Repository klonen

```bash
git clone <dein-repo-link>
cd medical_chatbot

python -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows

pip install -r requirements.txt

Du benÃ¶tigst einen Hugging Face Token fÃ¼r Llama 3.1:

Gehe zu: https://huggingface.co/settings/tokens
Erstelle einen Read-Token
Setze ihn in der Konsole:

export HF_TOKEN=hf_xxxxxxxxxxxxxxxx


5. Anwendung starten
ğŸŒ Webinterface starten

python web_rag_llama3.py

ğŸ› ï¸ Admin-Interface starten

python admin_interface.py

ğŸ§ª Nutzung
Chatbot-Befehle
Normale Frage stellen: Was ist eine Grippe?
Wissen speichern: Tippe lerne das nach einer Antwort
Admin-Interface
Alle EintrÃ¤ge anzeigen
Neue EintrÃ¤ge hinzufÃ¼gen
EintrÃ¤ge lÃ¶schen
In Wissensdatenbank suchen

ğŸ“ Projektstruktur

medical_chatbot/
â”œâ”€â”€ README.md               # Diese Datei
â”œâ”€â”€ requirements.txt        # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ medical_data.json       # Trainingsdaten
â”œâ”€â”€ train_medical_bot.py    # Lokales Training (optional)
â”œâ”€â”€ chatbot_cli.py          # Konsolen-Chatbot
â”œâ”€â”€ chatbot_web.py          # Einfaches Webinterface
â”œâ”€â”€ hybrid_rag.py           # RAG mit lokalem Retrieval
â”œâ”€â”€ web_rag_llama3.py       # Haupt-Webinterface mit Llama 3.1
â”œâ”€â”€ admin_interface.py      # Admin-Tool zur Wissensverwaltung
â””â”€â”€ chroma_db/              # Lokale Vektordatenbank (wird automatisch erstellt)

ğŸ§  Modellinformationen
Verwendete Modelle
Embedding: all-MiniLM-L6-v2 (lokal)
Generatives Modell: meta-llama/Llama-3.1-8B-Instruct (Cloud)
Basis-Modell: SpookyFab/tinyllama-pretrained-custom (lokal trainierbar)
Modellzugriff freischalten
Gehe zu: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
Klicke auf â€Access repositoryâ€œ
Akzeptiere die Nutzungsbedingungen

ğŸ“š Datenquellen
Ã–ffentliche medizinische DatensÃ¤tze
Wikipedia: Medizin-Kapitel
Eigene Erfahrungen aus Chats (mit Lernmodus)

ğŸ›¡ï¸ Datenschutz
Alle Retrieval-Prozesse laufen lokal
Fragen werden nicht dauerhaft gespeichert
Antworten werden nicht extern archiviert
Lernmodus speichert nur explizit genehmigte Inhalte

ğŸ§¾ NÃ¤chste Schritte
Wenn du das Projekt erweitern willst:

Mehr medizinische Daten hinzufÃ¼gen
Weitere Experten-Bots bauen (Recht, Technik, etc.)
Deployment in der Cloud (AWS, Google Cloud)
API-Schnittstelle bereitstellen

ğŸ¤ Mitwirken
Repository forken
Branch erstellen (feature/NeueFunktion)
Ã„nderungen committen
Pull Request erstellen

ğŸ“„ Lizenz
MIT License â€“ siehe LICENSE Datei

ğŸ“ Kontakt
Bei Fragen oder Problemen: Erstelle ein Issue im Repository.



